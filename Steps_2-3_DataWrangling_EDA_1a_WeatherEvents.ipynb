{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling for the weather-airports project.\n",
    "\n",
    "Part 1a: Weather events\n",
    "\n",
    "Weather data: https://www.kaggle.com/sobhanmoosavi/us-weather-events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pytz\n",
    "import sys\n",
    "sys.path.append('D:/Springboard_DataSci/Assignments/Lib')\n",
    "import TimeTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR = 2016\n",
    "END_YEAR = 2019\n",
    "START_TIME_LOCAL = 'StartTimeLocal'\n",
    "END_TIME_LOCAL = 'EndTimeLocal'\n",
    "START_OR_END = 'StartOrEnd'\n",
    "\n",
    "save_data = True#; save_data = False #whether to save the resulting data\n",
    "pd.options.mode.chained_assignment = None #turns off warnings for data replacements\n",
    "showHistogram = True#; showHistogram = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing weather events CSV file\n"
     ]
    }
   ],
   "source": [
    "'''Data collection'''\n",
    "stopwatch = TimeTracker.TimeTracker()\n",
    "# Get all the data.\n",
    "path = r'D:\\Springboard_DataSci\\Capstone_2\\data'\n",
    "os.chdir(path)\n",
    "print('Importing weather events CSV file')\n",
    "weather_events = pd.read_csv('US_WeatherEvents_2016-2019.csv', parse_dates=[3,4], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any duplicates dropped? False\n"
     ]
    }
   ],
   "source": [
    "# Check for any duplicates.\n",
    "nRows = len(weather_events.index)\n",
    "weather_events.drop_duplicates(inplace=True)\n",
    "print('Any duplicates dropped?', len(weather_events.index)!=nRows) #No duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Strategy\n",
    "\n",
    "To analyze the flights, we need to select our airports. Flights can be affected\n",
    "by issues at either the departure airport or the arrival airport.\n",
    "\n",
    "Let's think about the level of complexity that this could induce. Consider a\n",
    "directed graph with N nodes and V vertices, where every node is connected in\n",
    "both directions to every other node. The nodes represent the airports, and the\n",
    "vertices represent flights between the airports. There are V(N) = N(N-1)\n",
    "vertices in this graph. Here is a table of that function with N = 1, 2,..., 10:\n",
    "\n",
    "N  V<br>\n",
    "1  0<br>\n",
    "2  2<br>\n",
    "3  6<br>\n",
    "4  12<br>\n",
    "5  20<br>\n",
    "6  30<br>\n",
    "7  42<br>\n",
    "8  56<br>\n",
    "9  72<br>\n",
    "10  90<br>\n",
    "\n",
    "This scales up quickly! With over 25 million flights with which to work, perhaps\n",
    "it is better to choose just a few airports and develop our model as a proof of concept.\n",
    "\n",
    "There are a number of ways we could justify choosing our airports. We could take\n",
    "the list of busiest airports and pick the top few. However, we would like to pick\n",
    "airports that are not just busy but have different climates with different type of\n",
    "weather events. For instance, northern states are more likely to get snowstorms in the\n",
    "winter, southeastern states are more likely to get pop-up thunderstorms in the\n",
    "summer, and western states are often unaffected by weather entirely. We also\n",
    "need to keep the number of airports chosen low as discussed earlier. Finally,\n",
    "we would like to choose airports that are major hubs and important international\n",
    "gateways.\n",
    "\n",
    "Based on these factors, we choose four airports:\n",
    "\n",
    "(1) Atlanta: The world's busiest airport for years is home to Delta Airlines.\n",
    "Air traffic there is occasionally disrupted by rain.\n",
    "\n",
    "(2) New York-JFK: A hub for American and Delta Airlines and a major gateway\n",
    "to Europe, JFK Airport routinely has to contend with snowstorms in the winter.\n",
    "\n",
    "(3) Chicago O'Hare: The busiest airport in the region is also sometimes impacted\n",
    "by winter weather. It has recently undergone a multi-billion-dollar makeover\n",
    "that realigned many of its runways.\n",
    "\n",
    "(4) Miami: Another American hub, Miami enjoys warm weather year-round, but\n",
    "summertime storms are common. It is a gateway to South America."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRPORTS = ['ATL','JFK','ORD','MIA']\n",
    "ICAO_codes = ('K'+pd.Series(AIRPORTS)).to_list()\n",
    "Airport_names = ['Atlanta', 'JFK', 'O\\'Hare', 'Miami']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Atlanta weather events:\n",
      "Rain             2021\n",
      "Fog               163\n",
      "Precipitation      46\n",
      "Snow               40\n",
      "Hail                8\n",
      "Storm               7\n",
      "Name: Type, dtype: int64\n",
      "Light       1510\n",
      "Moderate     459\n",
      "Heavy        136\n",
      "Severe       126\n",
      "UNK           46\n",
      "Other          8\n",
      "Name: Severity, dtype: int64\n",
      "Latitude, Longitude: [33.6301] [-84.4418]\n",
      "\n",
      "JFK weather events:\n",
      "Rain             1690\n",
      "Snow              249\n",
      "Fog               211\n",
      "Precipitation      26\n",
      "Storm              15\n",
      "Hail               14\n",
      "Cold                3\n",
      "Name: Type, dtype: int64\n",
      "Light       1458\n",
      "Moderate     426\n",
      "Severe       183\n",
      "Heavy        101\n",
      "UNK           26\n",
      "Other         14\n",
      "Name: Severity, dtype: int64\n",
      "Latitude, Longitude: [40.6392] [-73.7639]\n",
      "\n",
      "O'Hare weather events:\n",
      "Rain             1903\n",
      "Snow              566\n",
      "Fog               235\n",
      "Precipitation      44\n",
      "Hail               10\n",
      "Storm               8\n",
      "Cold                3\n",
      "Name: Type, dtype: int64\n",
      "Light       1859\n",
      "Moderate     578\n",
      "Severe       142\n",
      "Heavy        136\n",
      "UNK           44\n",
      "Other         10\n",
      "Name: Severity, dtype: int64\n",
      "Latitude, Longitude: [41.9875] [-87.9319]\n",
      "\n",
      "Miami weather events:\n",
      "Rain             2329\n",
      "Precipitation     123\n",
      "Fog                51\n",
      "Cold                4\n",
      "Storm               4\n",
      "Name: Type, dtype: int64\n",
      "Light       1706\n",
      "Moderate     423\n",
      "Heavy        214\n",
      "UNK          123\n",
      "Severe        45\n",
      "Name: Severity, dtype: int64\n",
      "Latitude, Longitude: [25.788] [-80.3169]\n"
     ]
    }
   ],
   "source": [
    "'''Explore the weather data.'''\n",
    "weather_ATL = weather_events[weather_events.AirportCode=='KATL']\n",
    "weather_JFK = weather_events[weather_events.AirportCode=='KJFK']\n",
    "weather_ORD = weather_events[weather_events.AirportCode=='KORD']\n",
    "weather_MIA = weather_events[weather_events.AirportCode=='KMIA']\n",
    "\n",
    "for i, airport in enumerate ([weather_ATL, weather_JFK, weather_ORD, weather_MIA]):\n",
    "    print('\\n' + Airport_names[i] + ' weather events:\\n' + str(airport.Type.value_counts()))\n",
    "    print(airport.Severity.value_counts())\n",
    "    print('Latitude, Longitude:', airport.LocationLat.unique(), airport.LocationLng.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four airports' latitudes and longitudes check out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventId                           W-1\n",
      "Type                             Snow\n",
      "Severity                        Light\n",
      "StartTime(UTC)    2016-01-06 23:14:00\n",
      "EndTime(UTC)      2016-01-07 00:34:00\n",
      "TimeZone                  US/Mountain\n",
      "AirportCode                      K04V\n",
      "LocationLat                   38.0972\n",
      "LocationLng                  -106.169\n",
      "City                         Saguache\n",
      "County                       Saguache\n",
      "State                              CO\n",
      "ZipCode                         81149\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Keep only the airports that we need.\n",
    "print(weather_events.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of weather events at our airports: 9773\n"
     ]
    }
   ],
   "source": [
    "# The airport codes are in four-lettered codes, with K at the start of US airports.\n",
    "weather_events = weather_events[weather_events.AirportCode.isin(ICAO_codes)]\n",
    "print('No. of weather events at our airports:', len(weather_events)) #9773, down from over 5 million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of useless index.\n",
    "weather_events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop useless columns.\n",
    "weather_events = weather_events.loc[:, 'Type':'AirportCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting weather start times to local time zones\n",
      "Converting weather end times to local time zones\n"
     ]
    }
   ],
   "source": [
    "# Convert datetimes and timezones to the right types of objects.\n",
    "weather_events['StartTime(UTC)'] = weather_events['StartTime(UTC)'].dt.tz_localize('utc')\n",
    "weather_events['EndTime(UTC)'] = weather_events['EndTime(UTC)'].dt.tz_localize('utc')\n",
    "weather_events['TimeZone'] = weather_events['TimeZone'].map(pytz.timezone)\n",
    "\n",
    "# Localize the times to their respective time zones.\n",
    "print('Converting weather start times to local time zones')\n",
    "weather_events[START_TIME_LOCAL] = weather_events.apply(\n",
    "    lambda row: row['StartTime(UTC)'].tz_convert(row['TimeZone']), axis=1)\n",
    "print('Converting weather end times to local time zones')\n",
    "weather_events[END_TIME_LOCAL] = weather_events.apply(\n",
    "    lambda row: row['EndTime(UTC)'].tz_convert(row['TimeZone']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recast the 4-lettered ICAO-code index into the 3-lettered IATA codes.\n",
    "weather_events.rename(columns={'AirportCode': 'Airport'}, inplace=True)\n",
    "weather_events.Airport = weather_events.Airport.apply(\n",
    "    lambda prefix: prefix[1:])\n",
    "\n",
    "# Drop useless columns.\n",
    "weather_events = weather_events[['Airport',START_TIME_LOCAL,END_TIME_LOCAL,'Type','Severity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper which us-weather-events.csv is based on, Hail is \"solid precipitation\n",
    "including ice pellets and hail.\" This is a problem. Sleet rarely measures more than a couple\n",
    "millimeters in diameter, but hail can grow up to several centimeters in diameter, making\n",
    "takeoff and landing too dangerous. Unfortunately, the paper combines these two different types\n",
    "of precipitation into one.\n",
    "\n",
    "It also defines Storm as \"the extremely windy condition, where the wind speed is at least\n",
    "60kmh [sic].\" That is about 40 mph. We need to rename \"Storm\" to \"Wind\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_events['Type'].replace('Storm', 'Wind', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can discern between hail and sleet in our dataset. First, get a list of every\n",
    "\"hail\" event and see what is immediately around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hail events: 32\n"
     ]
    }
   ],
   "source": [
    "hail_events = weather_events[weather_events.Type=='Hail']\n",
    "hailEventIndices = weather_events[weather_events.Type=='Hail'].index\n",
    "n_hail = len(hailEventIndices)\n",
    "print('Number of hail events:', n_hail)\n",
    "assert hailEventIndices[0]!=0 and hailEventIndices[-1]!=len(hailEventIndices)-1,\\\n",
    "    'Very first or last event is hail, need to account for this'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how long before the first immediately prior and following events.\n",
    "hail_prior_events = weather_events.loc[hailEventIndices-1].drop(START_TIME_LOCAL, axis=1) #drop for simplicity\n",
    "hail_next_events = weather_events.loc[hailEventIndices+1].drop(END_TIME_LOCAL, axis=1) #drop for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All events with hail codes have the same airport as their prior codes? True\n",
      "All events with hail codes have the same airport as their following codes? True\n"
     ]
    }
   ],
   "source": [
    "# Some of these might be between different airports. Need to check this.\n",
    "print('All events with hail codes have the same airport as their prior codes?',\n",
    "      all([hail_events['Airport'].iloc[i] == hail_prior_events['Airport'].iloc[i] for i in range(n_hail)]))\n",
    "print('All events with hail codes have the same airport as their following codes?',\n",
    "      all([hail_events['Airport'].iloc[i] == hail_next_events['Airport'].iloc[i] for i in range(n_hail)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can discern between hail and sleet in our dataset. First, get a list of every\n",
    "\"hail\" event and see what is immediately around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hailStartTimeGap values:\n",
      "0 days 00:00:00    26\n",
      "0 days 00:04:00     1\n",
      "0 days 01:20:00     1\n",
      "0 days 05:25:00     1\n",
      "0 days 21:41:00     1\n",
      "2 days 13:41:00     1\n",
      "4 days 18:58:00     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Good, no discrepancies. Next, let's check the time gaps.\n",
    "hailStartTimeGap = pd.Series([hail_events[START_TIME_LOCAL].iloc[i]\\\n",
    "                              - hail_prior_events[END_TIME_LOCAL].iloc[i] for i in range(n_hail)])\n",
    "hailEndTimeGap = pd.Series([hail_next_events[START_TIME_LOCAL].iloc[i]\\\n",
    "                            - hail_events[END_TIME_LOCAL].iloc[i] for i in range(n_hail)])\n",
    "print('\\nhailStartTimeGap values:\\n' + str(hailStartTimeGap.value_counts().sort_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hailEndTimeGap values:\n",
      "0 days 00:00:00    29\n",
      "0 days 00:04:00     1\n",
      "0 days 03:54:00     1\n",
      "1 days 10:07:00     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\nhailEndTimeGap values:\\n' + str(hailEndTimeGap.value_counts().sort_index()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of \"StartTimeGap\" and nearly all of \"EndTimeGap\"s are 0. We need to make a decision\n",
    "of when the cutoff will be for associating the current event with the next/prior one.\n",
    "\n",
    "To start, let's sample just the ones with startTimeGaps and endTimeGaps of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_StartTimeGap = hailStartTimeGap.apply(\n",
    "    lambda x: timedelta.total_seconds(x)/3600)\n",
    "hours_EndTimeGap = hailEndTimeGap.apply(\n",
    "    lambda x: timedelta.total_seconds(x)/3600)\n",
    "indeces_ZeroTimeGap = hours_StartTimeGap[hours_StartTimeGap==0].index.intersection(\n",
    "    hours_EndTimeGap[hours_EndTimeGap==0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prior event codes with zero time gap:\n",
      "Rain    17\n",
      "Snow     7\n",
      "Name: Type, dtype: int64\n",
      "\n",
      "Next event codes with zero time gap:\n",
      "Rain    19\n",
      "Snow     5\n",
      "Name: Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "hail_prior_events_ZeroTimeGap = hail_prior_events.iloc[indeces_ZeroTimeGap]\n",
    "hail_next_events_ZeroTimeGap = hail_next_events.iloc[indeces_ZeroTimeGap]\n",
    "print('\\nPrior event codes with zero time gap:\\n' + str(\n",
    "    hail_prior_events.iloc[indeces_ZeroTimeGap]['Type'].value_counts()))\n",
    "print('\\nNext event codes with zero time gap:\\n' + str(\n",
    "    hail_next_events.iloc[indeces_ZeroTimeGap]['Type'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Most of the surrounding codes are Rain, but a few are Snow. This doesn't give us\n",
    "any clear indication yet between sleet or hail.\n",
    "\n",
    "However, we can make some assumptions. First, if the weather code immediately prior\n",
    "or after is snow, this event marked \"hail\" is actually sleet. Let's start attempting\n",
    "to classify what the Hail code actually means. To start, we populate that entire\n",
    "column with NaN's, with our goal to accurately reduce them to zero. Assumptions\n",
    "are listed below.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAIL = 1\n",
    "SLEET = 0\n",
    "REM_NANS = 'Remaining unassigned Hail codes:'\n",
    "AHC = 'ActualHailCode' #so I don't have to keep rewriting this\n",
    "\n",
    "hailCodeNaNs = lambda df: df[AHC].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histHailCount(df):\n",
    "    month = df['Month']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(month-0.5, bins=12, range=[0,12], align='right', color='r')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Hail codes by month')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting unassigned Hail codes: 32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hail_events[AHC] = np.nan\n",
    "hail_events['Month'] = hail_events[START_TIME_LOCAL].apply(lambda t: t.month)\n",
    "print('Starting unassigned Hail codes:', hailCodeNaNs(hail_events)) #Starting count: 32\n",
    "if showHistogram:\n",
    "    histHailCount(hail_events[hail_events[AHC].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining unassigned Hail codes: 29\n"
     ]
    }
   ],
   "source": [
    "#1. Any Hail code in June, July, August, or September is probably hail.\n",
    "hail_events[AHC][(hail_events['Month'] >= 6) & (hail_events['Month'] <= 9)] = HAIL\n",
    "print(REM_NANS, hailCodeNaNs(hail_events)) #Remaining count: 198 (was 99; September excluded)\n",
    "nAn_indeces = hail_events[hail_events[AHC].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining unassigned Hail codes: 15\n"
     ]
    }
   ],
   "source": [
    "#2. Any Hail code within six hours of a Snow code is probably sleet.\n",
    "time_gap = 6\n",
    "events_prior = hail_prior_events.loc[nAn_indeces - 1]\n",
    "events_next = hail_next_events.loc[nAn_indeces + 1]\n",
    "events_prior.set_index(events_prior.index + 1, inplace=True)\n",
    "events_next.set_index(events_next.index - 1, inplace=True)\n",
    "\n",
    "boolean_prior = (events_prior[END_TIME_LOCAL] + timedelta(hours=time_gap) >=\\\n",
    "    hail_events[hail_events[AHC].isna()][START_TIME_LOCAL]) &\\\n",
    "    (events_prior['Type'] == 'Snow')\n",
    "boolean_next = (events_next[START_TIME_LOCAL] - timedelta(hours=time_gap) <=\\\n",
    "    hail_events[hail_events[AHC].isna()][END_TIME_LOCAL]) &\\\n",
    "    (events_next['Type'] == 'Snow')\n",
    "hail_events[AHC].loc[(boolean_prior | boolean_next)\\\n",
    "                     [(boolean_prior | boolean_next)].index] = SLEET\n",
    "print(REM_NANS, hailCodeNaNs(hail_events)) #Remaining count: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining unassigned Hail codes: 5\n",
      "Airports and months of remaining unassigned hail codes:\n",
      "Month    3  4\n",
      "Airport      \n",
      "ATL      3  0\n",
      "JFK      1  0\n",
      "ORD      0  1\n"
     ]
    }
   ],
   "source": [
    "#3. Any remaining Hail code in November, December, January, or February is probably sleet.\n",
    "hail_events[AHC][hail_events[AHC].isna() & (\n",
    "    (hail_events['Month'] >= 11) | (hail_events['Month'] <= 2))] = SLEET\n",
    "print(REM_NANS, hailCodeNaNs(hail_events)) #Remaining count: 5\n",
    "print('Airports and months of remaining unassigned hail codes:')\n",
    "remaining_hail_unknowns = hail_events[hail_events[AHC].isna()][['Airport','Month']]\n",
    "remaining_hail_unknowns_crosstab = pd.crosstab(\n",
    "    index=remaining_hail_unknowns['Airport'], columns=remaining_hail_unknowns['Month'])\n",
    "print(remaining_hail_unknowns_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Airport             StartTimeLocal\n",
      "1712     JFK  2019-03-10 06:19:00-04:00\n",
      "2481     ORD  2016-04-25 22:29:00-05:00\n",
      "5119     ATL  2016-03-03 13:28:00-05:00\n",
      "6195     ATL  2018-03-20 00:23:00-04:00\n",
      "6928     ATL  2019-03-25 20:20:00-04:00\n",
      "Remaining unassigned Hail codes: 0\n"
     ]
    }
   ],
   "source": [
    "# Only 5 codes remain. Let's just look them up. Hail is a warm-weather event; sleet is a cold-weather event.\n",
    "print(hail_events.loc[hail_events[AHC].isna(), ['Airport',START_TIME_LOCAL]])\n",
    "hail_events.loc[1712, AHC] = SLEET # 35 degrees F\n",
    "hail_events.loc[2481, AHC] = HAIL # 68F\n",
    "hail_events.loc[5119, AHC] = SLEET # 47F\n",
    "hail_events.loc[6195, AHC] = HAIL # 61F\n",
    "hail_events.loc[6928, AHC] = HAIL # 57F\n",
    "\n",
    "print('Remaining unassigned Hail codes:', hailCodeNaNs(hail_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining unassigned Hail codes: 0\n"
     ]
    }
   ],
   "source": [
    "# Now let's set the correct codes to sleet.\n",
    "weather_events.loc[hail_events[hail_events[AHC] == SLEET].index, 'Type'] = 'Sleet'\n",
    "print(REM_NANS, hailCodeNaNs(hail_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the hail codes are resolved. Remove the hail and other unneeded variables.\n",
    "del(HAIL, SLEET, hailEndTimeGap, hailEventIndices, hailStartTimeGap, hail_events, hail_next_events,\n",
    "    hail_next_events_ZeroTimeGap, hail_prior_events, hail_prior_events_ZeroTimeGap, hours_EndTimeGap,\n",
    "    hours_StartTimeGap, indeces_ZeroTimeGap, nAn_indeces, nRows, n_hail, time_gap, ICAO_codes,\n",
    "    weather_ATL, weather_JFK, weather_MIA, weather_ORD, boolean_next, boolean_prior, airport,\n",
    "    events_next, events_prior, remaining_hail_unknowns, remaining_hail_unknowns_crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate weather types and declare numeric variables for them.\n",
    "Begin transitioning to the daily_weather DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain             7943\n",
      "Snow              855\n",
      "Fog               660\n",
      "Precipitation     239\n",
      "Wind               34\n",
      "Sleet              26\n",
      "Cold               10\n",
      "Hail                6\n",
      "Name: Type, dtype: int64\n",
      "\n",
      "Severities for weather type Rain:\n",
      "Light       5931\n",
      "Moderate    1489\n",
      "Heavy        523\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Fog:\n",
      "Severe      452\n",
      "Moderate    208\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Snow:\n",
      "Light       602\n",
      "Moderate    189\n",
      "Heavy        64\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Precipitation:\n",
      "UNK    239\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Sleet:\n",
      "Other    26\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Wind:\n",
      "Severe    34\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Cold:\n",
      "Severe    10\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for weather type Hail:\n",
      "Other    6\n",
      "Name: Severity, dtype: int64\n",
      "\n",
      "Severities for all types:\n",
      " Light       6533\n",
      "Moderate    1886\n",
      "Heavy        587\n",
      "Severe       496\n",
      "UNK          239\n",
      "Other         32\n",
      "Name: Severity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "severity_value_counts = weather_events.Type.value_counts()\n",
    "print(severity_value_counts)\n",
    "if save_data:\n",
    "    severity_value_counts.to_csv('severity_value_counts.csv')\n",
    "for weatherType in weather_events.Type.unique():\n",
    "    print('\\nSeverities for weather type ' + weatherType + ':')\n",
    "    print(weather_events[weather_events.Type==weatherType].Severity.value_counts())\n",
    "print('\\nSeverities for all types:\\n', weather_events.Severity.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather codes\n",
    "\n",
    "Rain: Light, Moderate, Heavy<br>\n",
    "Fog: Moderate, Severe<br>\n",
    "Snow: Light, Moderate, Heavy<br>\n",
    "Precipitation: UNK (we need to fix this)<br>\n",
    "Hail: Other<br>\n",
    "Sleet: Other<br>\n",
    "Storm: Severe<br>\n",
    "Cold: Severe<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_width = len(weather_events.columns) #get this now before new columns added\n",
    "weather_events['HailCode'] = (weather_events.Type == 'Hail').map(int)\n",
    "weather_events['SleetCode'] = (weather_events.Type == 'Sleet').map(int)\n",
    "weather_events['HiWindCode'] = (weather_events.Type == 'Wind').map(int) #See later\n",
    "weather_events['ColdCode'] = (weather_events.Type == 'Cold').map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LightModerateHeavy = {'Light':1, 'Moderate':2, 'Heavy':3}\n",
    "weather_events['RainCode'] =\\\n",
    "    weather_events[weather_events.Type=='Rain'].Severity.map(LightModerateHeavy)\n",
    "weather_events.RainCode = weather_events.RainCode.fillna(value=0)\n",
    "weather_events['FogCode'] =\\\n",
    "    weather_events[weather_events.Type=='Fog'].Severity.map({'Moderate':1, 'Severe':2})\n",
    "weather_events.FogCode = weather_events.FogCode.fillna(value=0)\n",
    "weather_events['SnowCode'] =\\\n",
    "    weather_events[weather_events.Type=='Snow'].Severity.map(LightModerateHeavy)\n",
    "weather_events.SnowCode = weather_events.SnowCode.fillna(value=0)\n",
    "FIRST_WEATHER_CODE = (weather_events.columns)[DF_width] #needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of \"Precipitation\" XOR \"UNK:\" 0\n"
     ]
    }
   ],
   "source": [
    "#Fix the \"Precipitation\" and \"UNK\" entries.\n",
    "PRECIP = 'Precipitation'; UNK = 'UNK'\n",
    "print('Count of \"' + PRECIP + '\" XOR \"' + UNK + ':\"', len(weather_events\\\n",
    "    [((weather_events.Type == PRECIP) &\n",
    "      (weather_events.Severity != UNK)) |\n",
    "      ((weather_events.Type != PRECIP) &\n",
    "      (weather_events.Severity == UNK))].index)) #Perfect match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputing these events\n"
     ]
    }
   ],
   "source": [
    "print('\\nImputing these events')\n",
    "for airport in AIRPORTS:\n",
    "    airport_indeces = weather_events[weather_events.Airport == airport].index\n",
    "    if airport_indeces.size > 0:\n",
    "        assert np.max(np.diff(airport_indeces)) == 1,\\\n",
    "            'Indexes misaligned for ' + str(airport)\n",
    "        weather_current_airport = weather_events.loc[airport_indeces]\n",
    "        unknown_precip_indeces = weather_current_airport\\\n",
    "            [weather_current_airport.Type == PRECIP].index\n",
    "        if len(unknown_precip_indeces)==0:\n",
    "            print(airport, 'has no unknown precipitation')\n",
    "        else:\n",
    "            weather_before_indeces = unknown_precip_indeces-1\n",
    "            weather_after_indeces = unknown_precip_indeces+1\n",
    "            try:\n",
    "                weather_current_airport.loc[weather_before_indeces[0]]\n",
    "            except KeyError:\n",
    "                raise ValueError('First weather event at', airport, 'is unknown precip')\n",
    "            try:\n",
    "                weather_current_airport.loc[weather_after_indeces[-1]]\n",
    "            except KeyError:\n",
    "                raise ValueError('Last weather event at', airport, 'is unknown precip')\n",
    "            # Will need to code a workaround if either of these issues arise.\n",
    "        \n",
    "            # Impute based on averages from entries immediately before and after.\n",
    "            # Round up decimals to nearest integer.\n",
    "            imputation_values = np.ceil((\n",
    "                weather_current_airport.loc[weather_before_indeces, FIRST_WEATHER_CODE:].values\n",
    "                + weather_current_airport.loc[weather_after_indeces, FIRST_WEATHER_CODE:].values)/2).astype(int)\n",
    "            for i, index in enumerate(unknown_precip_indeces):\n",
    "                weather_events.loc[index, FIRST_WEATHER_CODE:] = imputation_values[i]\n",
    "    else:\n",
    "        print(airport, 'has no entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Airport', 'StartTimeLocal', 'EndTimeLocal', 'Type', 'Severity',\n",
      "       'HailCode', 'SleetCode', 'HiWindCode', 'ColdCode', 'RainCode',\n",
      "       'FogCode', 'SnowCode'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(weather_events.columns)\n",
    "weather_events.drop(['Type', 'Severity'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather events' impacts on flights do not end the moment the weather does. This is particularly\n",
    "a problem for snow, which does not run off like rain does. The remedy for this in our database\n",
    "is simple: Extend the length of the weather events.\n",
    "\n",
    "How long to extend each event is probably a project in its own right. For now, we will use the following:<br>\n",
    "Light snow: Lasts 1 hour after the event<br>\n",
    "Moderate snow: Lasts 2 hours after<br>\n",
    "Heavy snow: Lasts 6 hours<br>\n",
    "All other events: Lasts 15 minutes\n",
    "\n",
    "Snow: Begins 2 hours before the event<br>\n",
    "All others: Begins 15 minutes before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_time(row):\n",
    "    row[START_TIME_LOCAL] -= timedelta(minutes=[15, 60][row.SnowCode > 0])\n",
    "    row[END_TIME_LOCAL] += timedelta(hours=[1/2, 1, 2, 6][int(row.SnowCode)])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending weather events\n"
     ]
    }
   ],
   "source": [
    "print('Extending weather events')\n",
    "weather_events = weather_events.apply(\n",
    "    lambda row: adjust_time(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build timelines of each weather event. To start, we split up the dataframe by event type,\n",
    "then melt the dataframes on the airports and appropriate weather codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_start_end = weather_events.loc[:, :END_TIME_LOCAL]\n",
    "events_rain = pd.concat([airport_start_end, pd.DataFrame(weather_events.RainCode)], axis=1)[weather_events.RainCode > 0]\n",
    "events_sleet = pd.concat([airport_start_end, pd.DataFrame(weather_events.SleetCode)], axis=1)[weather_events.SleetCode > 0]\n",
    "events_wind = pd.concat([airport_start_end, pd.DataFrame(weather_events.HiWindCode)], axis=1)[weather_events.HiWindCode > 0]\n",
    "events_cold = pd.concat([airport_start_end, pd.DataFrame(weather_events.ColdCode)], axis=1)[weather_events.ColdCode > 0]\n",
    "events_fog = pd.concat([airport_start_end, pd.DataFrame(weather_events.FogCode)], axis=1)[weather_events.FogCode > 0]\n",
    "events_snow = pd.concat([airport_start_end, pd.DataFrame(weather_events.SnowCode)], axis=1)[weather_events.SnowCode > 0]\n",
    "events_hail = pd.concat([airport_start_end, pd.DataFrame(weather_events.HailCode)], axis=1)[weather_events.HailCode > 0]\n",
    "\n",
    "sort_events = lambda df: df.sort_values(by=['Airport', 'Time']).reset_index(drop=True)\n",
    "\n",
    "events_cold = sort_events(pd.melt(events_cold, id_vars=['Airport', 'ColdCode'], var_name=START_OR_END, value_name='Time'))\n",
    "events_fog = sort_events(pd.melt(events_fog, id_vars=['Airport', 'FogCode'], var_name=START_OR_END, value_name='Time'))\n",
    "events_hail = sort_events(pd.melt(events_hail, id_vars=['Airport', 'HailCode'], var_name=START_OR_END, value_name='Time'))\n",
    "events_rain = sort_events(pd.melt(events_rain, id_vars=['Airport', 'RainCode'], var_name=START_OR_END, value_name='Time'))\n",
    "events_sleet = sort_events(pd.melt(events_sleet, id_vars=['Airport', 'SleetCode'], var_name=START_OR_END, value_name='Time'))\n",
    "events_snow = sort_events(pd.melt(events_snow, id_vars=['Airport', 'SnowCode'], var_name=START_OR_END, value_name='Time'))\n",
    "events_wind = sort_events(pd.melt(events_wind, id_vars=['Airport', 'HiWindCode'], var_name=START_OR_END, value_name='Time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the binary codes--Cold, hail, sleet, and wind--the start and end times should alternate. Remove any code that doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Removes any consecutive and overlap codes (latter for start codes, former for end codes), and promote any\n",
    "overlapping codes to the higher code. Input df is assumed to be sorted by Airport and then Time.'''\n",
    "def remove_overlaps(df, code_column=1):\n",
    "    assert set(df[START_OR_END].unique()) == {START_TIME_LOCAL, END_TIME_LOCAL},\\\n",
    "        START_OR_END + ' column contains ' + str(set(df[START_OR_END].unique()))\\\n",
    "            + ', should only contain ' + str({START_TIME_LOCAL, END_TIME_LOCAL})\n",
    "    top_code = int(max(df.iloc[:, code_column]))\n",
    "    print('Attempting to balance on', df.columns[code_column])\n",
    "    \n",
    "    '''Combine consecutive codes with the same start/end and intensity values into one.'''\n",
    "    def combine_same_codes(df, code_column):\n",
    "        rows_to_drop = []\n",
    "        nrows = df.shape[0]\n",
    "        while True:\n",
    "            for code in range(1, top_code+1):\n",
    "                current_df = df[df.iloc[:, code_column] == code] #same code\n",
    "                for airport in (current_df.Airport.unique()):\n",
    "                    specific_df = current_df[current_df.Airport == airport] #same code and airport\n",
    "                    rows_waiting = 0\n",
    "                    for i in range(len(specific_df)):\n",
    "                        row_name = specific_df.iloc[i].name\n",
    "                        if specific_df[START_OR_END].iloc[i] == START_TIME_LOCAL:\n",
    "                            rows_waiting += 1 #We have an unresolved StartTimeLocal\n",
    "                            if rows_waiting > 1:\n",
    "                                rows_to_drop.append(row_name)\n",
    "                        else: #EndTimeLocal\n",
    "                            if rows_waiting > 1: #We have an unresolved EndTimeLocal\n",
    "                                rows_to_drop.append(row_name)\n",
    "                            rows_waiting -= 1\n",
    "            if nrows == df.shape[0]:\n",
    "                break\n",
    "            nrows = df.shape[0]\n",
    "        return sort_events(df.drop(index=rows_to_drop))\n",
    "        return sort_events(df.loc[:, :'Time'].drop(index=rows_to_drop))\n",
    "    \n",
    "    '''Move any lower code that is \"hidden\" by a higher code. For example, if a 1 code spans 5 AM - 7 AM,\n",
    "    and a 2 code spans 4 AM - 6 AM, the 1 code is set to start at 6:00:01 AM.'''\n",
    "    def move_embedded_codes(df, code_column):\n",
    "        ONES_LIVE = '1sLive'; TWOS_LIVE = '2sLive'; THREES_LIVE = '3sLive'\n",
    "        df[ONES_LIVE] = df[TWOS_LIVE] = df[THREES_LIVE] = 0\n",
    "        for i in range(len(df)):\n",
    "            current_code = int(df.iloc[i, code_column])\n",
    "            # Calculate whether the code is balanced (0) or not (1)\n",
    "            if df[START_OR_END].iloc[i] == START_TIME_LOCAL:\n",
    "                df[[ONES_LIVE, TWOS_LIVE, THREES_LIVE][current_code-1]].iloc[i] += 1\n",
    "            else:\n",
    "                df[[ONES_LIVE, TWOS_LIVE, THREES_LIVE][current_code-1]].iloc[i] -= 1\n",
    "            if i>0: # Creates cumulative sums\n",
    "                df.loc[i, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]] = df.loc[i, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]] +\\\n",
    "                    df.loc[i-1, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]]\n",
    "                    \n",
    "        for i in range(1, len(df)):\n",
    "            if df[START_OR_END].iloc[i] == END_TIME_LOCAL:\n",
    "                # A 2 ends, exposing an unresolved 1 (3 must not be live)\n",
    "                if all(df.loc[i-1, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]] == [1, 1, 0])\\\n",
    "                and all(df.loc[i, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]] == [1, 0, 0]):\n",
    "                    # Search backwards until we find a start code 1\n",
    "                    j = i-1\n",
    "                    while True:\n",
    "                        if df.iloc[j, code_column] == 1:\n",
    "                            break\n",
    "                        j -= 1\n",
    "                    # Move this 1 code start time to just after the 2 code end time\n",
    "                    df.loc[j, 'Time'] = df.loc[i, 'Time'] + timedelta(seconds=1)\n",
    "                \n",
    "                # A 3 ends, exposing an unresolved 2\n",
    "                elif all(df.loc[i-1, [TWOS_LIVE, THREES_LIVE]] == [1, 1])\\\n",
    "                and all(df.loc[i, [TWOS_LIVE, THREES_LIVE]] == [1, 0]):\n",
    "                    # Search backwards until we find a start code 1\n",
    "                    j = i-1\n",
    "                    while True:\n",
    "                        if df.iloc[j, code_column] == 2:\n",
    "                            break\n",
    "                        j -= 1\n",
    "                    # Move this 2 code start time to just after the 3 code end time\n",
    "                    df.loc[j, 'Time'] = df.loc[i, 'Time'] + timedelta(seconds=2)\n",
    "                \n",
    "                # A 3 ends, exposing an unresolved 1 (2 must not be live)\n",
    "                elif all(df.loc[i-1, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]] == [1, 0, 1])\\\n",
    "                and all(df.loc[i, [ONES_LIVE, TWOS_LIVE, THREES_LIVE]] == [1, 0, 0]):\n",
    "                    # Search backwards until we find a start code 1\n",
    "                    j = i-1\n",
    "                    while True:\n",
    "                        if df.iloc[j, code_column] == 1:\n",
    "                            break\n",
    "                        j -= 1\n",
    "                    # Move this 1 code start time to just after the 3 code end time\n",
    "                    df.loc[j, 'Time'] = df.loc[i, 'Time'] + timedelta(seconds=1)\n",
    "\n",
    "                # A 2 ends while a 3 is still live, and the 2 started before the 3\n",
    "                if all(df.loc[i-1, [TWOS_LIVE, THREES_LIVE]] == [1, 1])\\\n",
    "                and all(df.loc[i, [TWOS_LIVE, THREES_LIVE]] == [0, 1]):      \n",
    "                    j = i-1\n",
    "                    starting_index = None\n",
    "                    while True:\n",
    "                        if df.iloc[j, code_column] == 3:\n",
    "                            starting_index = j\n",
    "                            break\n",
    "                        if df.iloc[j, code_column] == 2:\n",
    "                            break\n",
    "                        j -= 1\n",
    "                    # Move this 2 code end time to just before the 3 code end time\n",
    "                    if starting_index != None:\n",
    "                        df.loc[i, 'Time'] = df.loc[starting_index, 'Time'] - timedelta(seconds=2)\n",
    "\n",
    "                # A 1 ends while a 2 is still live, and the 1 started before the 2\n",
    "                elif all(df.loc[i-1, [ONES_LIVE, TWOS_LIVE]] == [1, 1])\\\n",
    "                and all(df.loc[i, [ONES_LIVE, TWOS_LIVE]] == [0, 1]):\n",
    "                    j = i-1\n",
    "                    starting_index = None\n",
    "                    while True:\n",
    "                        if df.iloc[j, code_column] == 2:\n",
    "                            starting_index = j\n",
    "                            break\n",
    "                        if df.iloc[j, code_column] == 1:\n",
    "                            break\n",
    "                        j -= 1\n",
    "                    # Move this 1 code end time to just before the 2 code end time\n",
    "                    if starting_index != None:\n",
    "                        df.loc[i, 'Time'] = df.loc[starting_index, 'Time'] - timedelta(seconds=1)\n",
    "\n",
    "                # A 1 ends while a 3 is still live, and the 1 started before the 3\n",
    "                elif all(df.loc[i-1, [ONES_LIVE, THREES_LIVE]] == [1, 1])\\\n",
    "                and all(df.loc[i, [ONES_LIVE, THREES_LIVE]] == [0, 1]):\n",
    "                    j = i-1\n",
    "                    starting_index = None\n",
    "                    while True:\n",
    "                        if df.iloc[j, code_column] == 3:\n",
    "                            starting_index = j\n",
    "                            break\n",
    "                        if df.iloc[j, code_column] == 1:\n",
    "                            break\n",
    "                        j -= 1\n",
    "                    # Move this 1 code end time to just before the 3 code end time\n",
    "                    if starting_index != None:\n",
    "                        df.loc[i, 'Time'] = df.loc[starting_index, 'Time'] - timedelta(seconds=1)\n",
    "                \n",
    "        return sort_events(df.loc[:, :'Time'])\n",
    "    \n",
    "    '''Promote concurrent codes to the higher one.'''\n",
    "    def promote_codes(df, code_column):\n",
    "        '''Increase or decrease the counter depending on whether the time is a StartTime or EndTime.'''\n",
    "        def counter_update(df, code_column, counter, value):\n",
    "            if df.iloc[i, code_column] == value:\n",
    "                if df.loc[i, START_OR_END] == START_TIME_LOCAL:\n",
    "                    return counter + 1\n",
    "                return counter - 1\n",
    "            return counter\n",
    "        \n",
    "        twos_live = threes_live = 0\n",
    "        for i in range(len(df)):\n",
    "            current_value = df.iloc[i, code_column]\n",
    "            if current_value == 3:\n",
    "                threes_live = counter_update(df, code_column, threes_live, 3)\n",
    "            elif current_value == 2:\n",
    "                if threes_live > 0:\n",
    "                    df.iloc[i, code_column] = 3\n",
    "                else:\n",
    "                    twos_live = counter_update(df, code_column, twos_live, 2)\n",
    "            else:\n",
    "                if threes_live > 0:\n",
    "                    df.iloc[i, code_column] = 3\n",
    "                elif twos_live > 0:\n",
    "                    df.iloc[i, code_column] = 2\n",
    "        return df\n",
    "    \n",
    "    # Step 1: Combine events within the same code.\n",
    "    df = combine_same_codes(df, code_column)\n",
    "\n",
    "    if top_code > 1: # No embedded codes if only 1-codes exist\n",
    "        attempts = 1\n",
    "        # Step 2: Deal with overlaps if necessary. This may take several passes.\n",
    "        while True:\n",
    "            print('  Embedding attempts:', attempts)\n",
    "            df_updated = move_embedded_codes(df.copy(), code_column)\n",
    "            if df.equals(df_updated):\n",
    "                break\n",
    "            attempts += 1\n",
    "            df = df_updated\n",
    "\n",
    "        # Step 3: Promote embedded lower codes.\n",
    "        df = promote_codes(df, code_column)\n",
    "        \n",
    "        # Step 4: Do one more round of cleanup.\n",
    "        df = combine_same_codes(df, code_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to balance on ColdCode\n",
      "Attempting to balance on HailCode\n",
      "Attempting to balance on SleetCode\n",
      "Attempting to balance on HiWindCode\n"
     ]
    }
   ],
   "source": [
    "# Balance the 1-code sets (this is purely for completeness; they are already balanced)\n",
    "events_cold = remove_overlaps(events_cold.reset_index(drop=True))\n",
    "events_hail = remove_overlaps(events_hail.reset_index(drop=True))\n",
    "events_sleet = remove_overlaps(events_sleet.reset_index(drop=True))\n",
    "events_wind = remove_overlaps(events_wind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to balance on FogCode\n",
      "  Embedding attempts: 1\n",
      "  Embedding attempts: 2\n"
     ]
    }
   ],
   "source": [
    "# Balance the 2-code set\n",
    "events_fog = remove_overlaps(events_fog.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to balance on SnowCode\n",
      "  Embedding attempts: 1\n",
      "  Embedding attempts: 2\n",
      "  Embedding attempts: 3\n"
     ]
    }
   ],
   "source": [
    "# Balance the 3-code sets\n",
    "events_snow = remove_overlaps(events_snow.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to balance on RainCode\n",
      "  Embedding attempts: 1\n",
      "  Embedding attempts: 2\n",
      "  Embedding attempts: 3\n"
     ]
    }
   ],
   "source": [
    "events_rain = remove_overlaps(events_rain.reset_index(drop=True)) #takes awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n"
     ]
    }
   ],
   "source": [
    "# Save the balanced weather codes\n",
    "if save_data:\n",
    "    print('Saving data')\n",
    "    events_cold.to_csv('events_cold.csv', index=False)\n",
    "    events_hail.to_csv('events_hail.csv', index=False)\n",
    "    events_sleet.to_csv('events_sleet.csv', index=False)\n",
    "    events_wind.to_csv('events_wind.csv', index=False)\n",
    "    events_fog.to_csv('events_fog.csv', index=False)\n",
    "    events_snow.to_csv('events_snow.csv', index=False)\n",
    "    events_rain.to_csv('events_rain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: --- 5.18 minutes ---\n"
     ]
    }
   ],
   "source": [
    "print('Total runtime:', stopwatch.getElapsedTime())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
